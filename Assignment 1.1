Question 1: The various sources of BIG DATA are:
1)Archives
2)Docs
3)Media
4)Business Apps
5)Social Media
6)Public Web
7)Data storage
8)Machine Log Data
9)Sensor Data

Question 2: The three V's of Big data are:
                1.Velocity:	In this context, the speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development.
              
		2.Variety: 	The type and nature of the data. This helps people who analyze it to effectively use the resulting insight.

                3.Volume:	The quantity of generated and stored data. The size of the data determines the value and potential insight- and whether it can actually be considered big data or not.
              


Question 3:             In this world of cloud, one of the biggest features is the ability to scale. There are different ways to accomplish scaling, which is a transformation that enlarges or diminishes. One is vertical scaling and the other is horizontal scaling.
          
          Vertical scaling can essentially resize your server with no change to your code. It is the ability to increase the capacity of existing hardware or software by adding resources. Vertical scaling is limited by the fact that you can only get as big as the size of the server.
          Horizontal scaling affords the ability to scale wider to deal with traffic. It is the ability to connect multiple hardware or software entities, such as servers, so that they work as a single logical unit. This kind of scale cannot be implemented at a moment’s notice.
          
          
Question 4: 
Need for Hadoop:
Hadoop is an open source, Java-based programming framework that supports the processing of large data sets in a distributed computing environment and provides a reliable, scalable platform for storage and analysis. 

Working of Hadoop :	
It is based on Google File System or GFSGoogle File System (GFS or GoogleFS) is a proprietary distributed file system developed by Google to provide efficient, reliable access to data using large clusters of commodity hardware.Hadoop runs a number of applications on distributed systems with thousands of nodes involving petabytes of data.It has a distributed file system, called the Hadoop Distributed File System or HDFS, which enables fast data transfer among the nodes. It leverages a distributed computation framework called MapReduce.

 
